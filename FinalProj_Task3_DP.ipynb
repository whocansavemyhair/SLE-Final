{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Project_ Task 3 with DP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Libary & setting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:06:47.378891Z",
     "start_time": "2024-12-12T19:06:46.890760Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# multiple output in notebook without print()\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Class and Implement Policy Iteration and Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:06:58.045590Z",
     "start_time": "2024-12-12T19:06:53.248551Z"
    }
   },
   "source": [
    "### Define CartePoleDynamics including environment, parameter, state space setting \n",
    "class CartPoleDynamics:\n",
    "    def __init__(self):\n",
    "        # System parameters\n",
    "        self.M = 1.0  # Mass of the cart\n",
    "        self.m = 0.1  # Mass of the pole\n",
    "        self.g = -9.8  # Gravity\n",
    "        self.l = 0.5  # Length of the pole\n",
    "        self.mu_c = 0.0005  # Friction for the cart\n",
    "        self.mu_p = 0.000002  # Friction for the pole\n",
    "        self.delta_t = 0.02  # Time step\n",
    "        self.actions = [-10, 10]  # Available actions (forces in Newtons)\n",
    "        \n",
    "        # State space discretization\n",
    "        self.theta_boxes = np.array([-12, -6, -1, 0, 1, 6, 12]) * np.pi / 180  # radians\n",
    "        self.x_boxes = np.array([-2.4, -0.8, 0.8, 2.4])  # meters\n",
    "        self.theta_dot_boxes = np.array([-50, 0, 50]) * np.pi / 180  # radians/s\n",
    "        self.x_dot_boxes = np.array([-0.5, 0, 0.5])  # m/s\n",
    "        \n",
    "        # Define state space size\n",
    "        self.state_space_size = (\n",
    "            len(self.theta_boxes) + 1,\n",
    "            len(self.theta_dot_boxes) + 1,\n",
    "            len(self.x_boxes) + 1,\n",
    "            len(self.x_dot_boxes) + 1\n",
    "        )\n",
    "\n",
    "    def compute_accelerations(self, theta, theta_dot, x_dot, F):\n",
    "        \"\"\"Compute angular and linear accelerations based on the model.\"\"\"\n",
    "        sin_theta = np.sin(theta)\n",
    "        cos_theta = np.cos(theta)\n",
    "        \n",
    "        # Calculate angular acceleration (theta_ddot)\n",
    "        numerator = (self.g * sin_theta + \n",
    "                    cos_theta * ((-F - self.m * self.l * theta_dot**2 * sin_theta + \n",
    "                                self.mu_c * np.sign(x_dot)) / (self.M + self.m)) - \n",
    "                    self.mu_p * theta_dot / (self.m * self.l))\n",
    "        denominator = self.l * (4.0/3.0 - (self.m * cos_theta**2) / (self.M + self.m))\n",
    "        theta_ddot = numerator / denominator\n",
    "        \n",
    "        # Calculate linear acceleration (x_ddot)\n",
    "        x_ddot = (F + self.m * self.l * (theta_dot**2 * sin_theta - theta_ddot * cos_theta) - \n",
    "                 self.mu_c * np.sign(x_dot)) / (self.M + self.m)\n",
    "        \n",
    "        return theta_ddot, x_ddot\n",
    "    \n",
    "    def update_state(self, state, action):\n",
    "        \"\"\"Update the state using Euler integration\"\"\"\n",
    "        theta, theta_dot, x, x_dot = state\n",
    "        F = action\n",
    "        \n",
    "        theta_ddot, x_ddot = self.compute_accelerations(theta, theta_dot, x_dot, F)\n",
    "        \n",
    "        # Update using Euler integration\n",
    "        x_dot += self.delta_t * x_ddot\n",
    "        x += self.delta_t * x_dot\n",
    "        theta_dot += self.delta_t * theta_ddot\n",
    "        theta += self.delta_t * theta_dot\n",
    "        \n",
    "        return np.array([theta, theta_dot, x, x_dot])\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Discretize a continuous state based on provided thresholds\"\"\"\n",
    "        theta, theta_dot, x, x_dot = state\n",
    "        \n",
    "        theta_idx = np.digitize(theta, self.theta_boxes, right=True)\n",
    "        theta_dot_idx = np.digitize(theta_dot, self.theta_dot_boxes, right=True)\n",
    "        x_idx = np.digitize(x, self.x_boxes, right=True)\n",
    "        x_dot_idx = np.digitize(x_dot, self.x_dot_boxes, right=True)\n",
    "        \n",
    "        return (theta_idx, theta_dot_idx, x_idx, x_dot_idx)\n",
    "        \n",
    "    def is_state_valid(self, state):\n",
    "        \"\"\"Check if the state is within valid bounds\"\"\"\n",
    "        theta, _, x, _ = state\n",
    "        return (abs(theta) <= 12 * np.pi / 180 and abs(x) <= 2.4)\n",
    "\n",
    "### Define Dynamic Programming class for Implementing Algorithm\n",
    "class DynamicProgramming:\n",
    "    def __init__(self):\n",
    "        self.dynamics = CartPoleDynamics()\n",
    "        \n",
    "    \"\"\" Policy Iteration Algorithm\"\"\"    \n",
    "    def policy_iteration(self, gamma=0.99, threshold=1e-5):\n",
    "        \"\"\"Policy Iteration Algorithm with Policy Evaluation and Policy Improvement\"\"\"\n",
    "        # Initialize policy and value function\n",
    "        policy = np.random.choice([0, 1], size=self.dynamics.state_space_size)\n",
    "        value_function = np.zeros(self.dynamics.state_space_size)\n",
    "        \n",
    "        while True:\n",
    "            # Policy Evaluation\n",
    "            while True:\n",
    "                delta = 0\n",
    "                for theta_idx in range(self.dynamics.state_space_size[0]):\n",
    "                    for theta_dot_idx in range(self.dynamics.state_space_size[1]):\n",
    "                        for x_idx in range(self.dynamics.state_space_size[2]):\n",
    "                            for x_dot_idx in range(self.dynamics.state_space_size[3]):\n",
    "                                state = (theta_idx, theta_dot_idx, x_idx, x_dot_idx)\n",
    "                                action = self.dynamics.actions[policy[state]]\n",
    "                                value = value_function[state]\n",
    "                                new_value = self._compute_state_value(state, action, value_function, gamma)\n",
    "                                value_function[state] = new_value\n",
    "                                delta = max(delta, abs(value - new_value))\n",
    "                \n",
    "                if delta < threshold:\n",
    "                    break\n",
    "            \n",
    "            # Policy Improvement\n",
    "            policy_stable = True\n",
    "            for theta_idx in range(self.dynamics.state_space_size[0]):\n",
    "                for theta_dot_idx in range(self.dynamics.state_space_size[1]):\n",
    "                    for x_idx in range(self.dynamics.state_space_size[2]):\n",
    "                        for x_dot_idx in range(self.dynamics.state_space_size[3]):\n",
    "                            state = (theta_idx, theta_dot_idx, x_idx, x_dot_idx)\n",
    "                            old_action = policy[state]\n",
    "                            \n",
    "                            # Find best action\n",
    "                            action_values = []\n",
    "                            for action_idx, action in enumerate(self.dynamics.actions):\n",
    "                                value = self._compute_state_value(state, action, value_function, gamma)\n",
    "                                action_values.append(value)\n",
    "                            \n",
    "                            best_action = np.argmax(action_values)\n",
    "                            policy[state] = best_action\n",
    "                            \n",
    "                            if old_action != best_action:\n",
    "                                policy_stable = False\n",
    "            \n",
    "            if policy_stable:\n",
    "                break\n",
    "                \n",
    "        return policy, value_function\n",
    "    \n",
    "    ### Value Iteration Algorithm\n",
    "    def value_iteration(self, gamma=0.99, threshold=1e-5):\n",
    "        \"\"\"Value Iteration Algorithm\"\"\"\n",
    "        value_function = np.zeros(self.dynamics.state_space_size)\n",
    "        policy = np.zeros(self.dynamics.state_space_size, dtype=int)\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for theta_idx in range(self.dynamics.state_space_size[0]):\n",
    "                for theta_dot_idx in range(self.dynamics.state_space_size[1]):\n",
    "                    for x_idx in range(self.dynamics.state_space_size[2]):\n",
    "                        for x_dot_idx in range(self.dynamics.state_space_size[3]):\n",
    "                            state = (theta_idx, theta_dot_idx, x_idx, x_dot_idx)\n",
    "                            value = value_function[state]\n",
    "                            \n",
    "                            # Find maximum value over all actions\n",
    "                            action_values = []\n",
    "                            for action in self.dynamics.actions:\n",
    "                                new_value = self._compute_state_value(state, action, value_function, gamma)\n",
    "                                action_values.append(new_value)\n",
    "                            \n",
    "                            value_function[state] = max(action_values)\n",
    "                            policy[state] = np.argmax(action_values)\n",
    "                            delta = max(delta, abs(value - value_function[state]))\n",
    "            \n",
    "            if delta < threshold:\n",
    "                break\n",
    "                \n",
    "        return policy, value_function\n",
    "    \n",
    "    def _compute_state_value(self, state, action, value_function, gamma):\n",
    "        \"\"\"Helper method to compute value for a state-action pair\"\"\"\n",
    "        theta_idx, theta_dot_idx, x_idx, x_dot_idx = state\n",
    "        \n",
    "        # Convert discrete state to continuous\n",
    "        continuous_state = [\n",
    "            self.dynamics.theta_boxes[theta_idx - 1] if theta_idx > 0 else -np.inf,\n",
    "            self.dynamics.theta_dot_boxes[theta_dot_idx - 1] if theta_dot_idx > 0 else -np.inf,\n",
    "            self.dynamics.x_boxes[x_idx - 1] if x_idx > 0 else -np.inf,\n",
    "            self.dynamics.x_dot_boxes[x_dot_idx - 1] if x_dot_idx > 0 else -np.inf\n",
    "        ]\n",
    "        \n",
    "        # Get next state\n",
    "        next_state_continuous = self.dynamics.update_state(continuous_state, action)\n",
    "        next_state = self.dynamics.discretize_state(next_state_continuous)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = 1 if self.dynamics.is_state_valid(next_state_continuous) else 0\n",
    "        \n",
    "        # Compute value\n",
    "        return reward + gamma * value_function[next_state]\n",
    "\n",
    "    def plot_results(self, policy, value_function, method_name, fixed_theta_dot_idx, fixed_x_dot_idx):\n",
    "        \"\"\"Plot value function and policy for a given slice of state space\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot value function\n",
    "        value_slice = value_function[:, fixed_theta_dot_idx, :, fixed_x_dot_idx]\n",
    "        im1 = ax1.imshow(value_slice, extent=[self.dynamics.x_boxes[0], \n",
    "                                            self.dynamics.x_boxes[-1],\n",
    "                                            self.dynamics.theta_boxes[0],\n",
    "                                            self.dynamics.theta_boxes[-1]],\n",
    "                        aspect='auto', origin='lower', cmap='coolwarm')\n",
    "        plt.colorbar(im1, ax=ax1, label='Value')\n",
    "        ax1.set_title(f'Value Function ({method_name})')\n",
    "        ax1.set_xlabel('Cart Position (x) [m]')\n",
    "        ax1.set_ylabel('Pole Angle (θ) [rad]')\n",
    "        \n",
    "        # Plot policy\n",
    "        policy_slice = policy[:, fixed_theta_dot_idx, :, fixed_x_dot_idx]\n",
    "        im2 = ax2.imshow(policy_slice, extent=[self.dynamics.x_boxes[0],\n",
    "                                             self.dynamics.x_boxes[-1],\n",
    "                                             self.dynamics.theta_boxes[0],\n",
    "                                             self.dynamics.theta_boxes[-1]],\n",
    "                        aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(im2, ax=ax2, label='Action Index')\n",
    "        ax2.set_title(f'Policy ({method_name})')\n",
    "        ax2.set_xlabel('Cart Position (x) [m]')\n",
    "        ax2.set_ylabel('Pole Angle (θ) [rad]')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Create instance of DP solver\n",
    "    dp = DynamicProgramming()\n",
    "    \n",
    "    # Run policy iteration\n",
    "    pi_policy, pi_value = dp.policy_iteration()\n",
    "    \n",
    "    # Run value iteration\n",
    "    vi_policy, vi_value = dp.value_iteration()\n",
    "    \n",
    "    # Plot results for both methods\n",
    "    fixed_states = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]  # Different combinations of theta_dot and x_dot\n",
    "    for theta_dot_idx, x_dot_idx in fixed_states:\n",
    "        # Plot policy iteration results\n",
    "        dp.plot_results(pi_policy, pi_value, f\"Policy Iteration with θ_dot: {theta_dot_boxes[theta_dot_idx]*180/np.pi} (degree), x_dot: {x_dot_boxes[x_dot_idx]}\", theta_dot_idx, x_dot_idx)\n",
    "        \n",
    "        # Plot value iteration results\n",
    "        dp.plot_results(vi_policy, vi_value, f\"Value Iteration with θ_dot: {theta_dot_boxes[theta_dot_idx]*180/np.pi} (degree), x_dot: {x_dot_boxes[x_dot_idx]}\", theta_dot_idx, x_dot_idx)\n",
    "\n",
    "\n",
    "### Algorithm Implementation and Plotting\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta_dot_boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 239\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m### Algorithm Implementation and Plotting\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 239\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 231\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    228\u001B[0m fixed_states \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m), (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m), (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m)]  \u001B[38;5;66;03m# Different combinations of theta_dot and x_dot\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m theta_dot_idx, x_dot_idx \u001B[38;5;129;01min\u001B[39;00m fixed_states:\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;66;03m# Plot policy iteration results\u001B[39;00m\n\u001B[0;32m--> 231\u001B[0m     dp\u001B[38;5;241m.\u001B[39mplot_results(pi_policy, pi_value, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPolicy Iteration with θ_dot: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtheta_dot_boxes[theta_dot_idx]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m180\u001B[39m\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39mpi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (degree), x_dot: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_dot_boxes[x_dot_idx]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, theta_dot_idx, x_dot_idx)\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# Plot value iteration results\u001B[39;00m\n\u001B[1;32m    234\u001B[0m     dp\u001B[38;5;241m.\u001B[39mplot_results(vi_policy, vi_value, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValue Iteration with θ_dot: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtheta_dot_boxes[theta_dot_idx]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m180\u001B[39m\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39mpi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (degree), x_dot: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_dot_boxes[x_dot_idx]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, theta_dot_idx, x_dot_idx)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'theta_dot_boxes' is not defined"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
