{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T21:12:15.055272Z",
     "start_time": "2024-12-11T21:12:15.049368Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import gym\n",
    "from PyQt5.QtCore.QProcess import state\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print(f\"gymnasium Version: {gymnasium.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "print(f\"numpy {np.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-15.1.1-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.16.2\n",
      "Keras Version: 3.6.0\n",
      "gymnasium Version: 0.28.1\n",
      "\n",
      "Python 3.9.19 (main, May  6 2024, 14:39:30) \n",
      "[Clang 14.0.6 ]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.0.2\n",
      "SciPy 1.10.1\n",
      "numpy 1.24.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:35:12.130078Z",
     "start_time": "2024-12-11T22:35:12.078542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "n_actions = 2\n",
    "n_states = 24  \n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 1\n",
    "epochs = 1000\n",
    "\n",
    "theta_bins = [-12, -6, -1, 0, 1, 6, 12]  # θ: 0, ±1, ±6, ±12\n",
    "x_bins = [-2.4, -0.8, 0.8, 2.4]  # x: ±0.8, ±2.4 \n",
    "theta_dot_bins = [-50, 50]  # θ̇: ±50, inf\n",
    "x_dot_bins = [-0.5, 0.5]  # ẋ: ±0.5, inf\n",
    "\n",
    "# Q table\n",
    "Q = np.zeros((len(theta_bins)-1, len(x_bins)-1, len(theta_dot_bins)-1,\n",
    "              len(x_dot_bins)-1, n_actions))\n",
    "\n",
    "def discretize_state(state):\n",
    "    theta, x, theta_dot, x_dot = state\n",
    "    theta_idx = np.digitize(theta, theta_bins) - 1\n",
    "    x_idx = np.digitize(x, x_bins) - 1\n",
    "    theta_dot_idx = np.digitize(theta_dot, theta_dot_bins) - 1\n",
    "    x_dot_idx = np.digitize(x_dot, x_dot_bins) - 1\n",
    "    \n",
    "    return (theta_idx, x_idx, theta_dot_idx, x_dot_idx)\n",
    "\n",
    "def train():\n",
    "    rewards = []\n",
    "    for episode in range(epochs):\n",
    "        state,_ = env.reset()\n",
    "        if isinstance(state, dict):\n",
    "            state = state['state']\n",
    "        state = discretize_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # epsilon-greedy\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  \n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            force = 10 if action == 1 else -10\n",
    "            next_state, reward, done, _, _ = env.step(force)\n",
    "            if isinstance(next_state, dict):\n",
    "                next_state = next_state['state']\n",
    "            next_state = discretize_state(next_state)\n",
    "\n",
    "            # update Q\n",
    "            Q[state][action] = Q[state][action] + learning_rate * (\n",
    "                reward + discount_factor * np.max(Q[next_state]) - Q[state][action]\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{epochs}, Total reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "rewards = train()\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Q-Learning Performance on CartPole')\n",
    "plt.show()\n",
    "\n",
    "def test():\n",
    "    state,_ = env.reset()\n",
    "    if isinstance(state, dict):\n",
    "        state = state['state']\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state])  \n",
    "        force = 10 if action == 1 else -10\n",
    "        next_state, reward, done, _, _ = env.step(force)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        if isinstance(next_state, dict):\n",
    "            next_state = next_state['state']\n",
    "        next_state = discretize_state(next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Test Total Reward: {total_reward}\")\n",
    "\n",
    "test()"
   ],
   "id": "578fd6d67d42a38f",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "-10 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 71\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Total reward: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_reward\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m rewards\n\u001B[0;32m---> 71\u001B[0m rewards \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(rewards)\n\u001B[1;32m     74\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpisodes\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 51\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(Q[state])\n\u001B[1;32m     50\u001B[0m force \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m action \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[0;32m---> 51\u001B[0m next_state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforce\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_state, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m     53\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m next_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/school_project-Lin/lib/python3.9/site-packages/gym/wrappers/time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/school_project-Lin/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/school_project-Lin/lib/python3.9/site-packages/gym/wrappers/env_checker.py:37\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_step_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/school_project-Lin/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:214\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[0;34m(env, action)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[0;32m--> 214\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    216\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    217\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/school_project-Lin/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:132\u001B[0m, in \u001B[0;36mCartPoleEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    131\u001B[0m     err_msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maction\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(action)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) invalid\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mcontains(action), err_msg\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCall reset before using step method.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    134\u001B[0m     x, x_dot, theta, theta_dot \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\n",
      "\u001B[0;31mAssertionError\u001B[0m: -10 (<class 'int'>) invalid"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
